#!/usr/bin/env python3
# /// script
# dependencies = []
# ///
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†è„šæœ¬
ä»å®Œæ•´çš„ GitHub ä»“åº“ JSON æ•°æ®ä¸­æå–å…³é”®æ–‡æœ¬ä¿¡æ¯ï¼Œç”Ÿæˆé€‚åˆ LLM åˆ†æçš„ç²¾ç®€æ•°æ®ã€‚

æ ¸å¿ƒç›®çš„ï¼š
- å»é™¤æ— ç”¨çš„ JSON å…ƒæ•°æ®ï¼ˆAPI URLsã€SHA å€¼ã€ç¼–ç ä¿¡æ¯ç­‰ï¼‰
- æå–æœ‰ä»·å€¼çš„æ–‡æœ¬å†…å®¹ï¼ˆå®Œæ•´ä¿ç•™ï¼Œä¸æˆªæ–­ï¼‰
- ç”Ÿæˆç»“æ„æ¸…æ™°çš„åˆ†æå°±ç»ªæ•°æ®

è¿è¡Œæ–¹å¼ï¼šuv run scripts/prepare_for_analysis.py <json_file>
"""

import json
import sys
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional


def extract_basic_info(data: Dict) -> Dict:
    """æå–åŸºç¡€ä¿¡æ¯ï¼ˆåªä¿ç•™æœ‰ä»·å€¼çš„å­—æ®µï¼Œå»é™¤ API URLs ç­‰æ— ç”¨ä¿¡æ¯ï¼‰"""
    basic = data.get('basic_info', {})
    return {
        'name': basic.get('name', ''),
        'full_name': basic.get('full_name', ''),
        'description': basic.get('description', ''),
        'homepage': basic.get('homepage', ''),
        'html_url': basic.get('html_url', ''),
        'stars': basic.get('stargazers_count', 0),
        'forks': basic.get('forks_count', 0),
        'watchers': basic.get('watchers_count', 0),
        'open_issues': basic.get('open_issues_count', 0),
        'language': basic.get('language', ''),
        'license': basic.get('license', {}).get('name', '') if basic.get('license') else '',
        'topics': basic.get('topics', []),
        'created_at': basic.get('created_at', ''),
        'updated_at': basic.get('updated_at', ''),
        'pushed_at': basic.get('pushed_at', ''),
        'default_branch': basic.get('default_branch', ''),
        'size_kb': basic.get('size', 0),
        'is_fork': basic.get('fork', False),
        'archived': basic.get('archived', False),
    }


def extract_readme(data: Dict) -> str:
    """æå–å®Œæ•´çš„ README å†…å®¹"""
    readme = data.get('readme', {})
    return readme.get('decoded_content', '')


def extract_issues(data: Dict) -> List[Dict]:
    """æå– Issues ä¿¡æ¯ï¼ˆå®Œæ•´ä¿ç•™æ­£æ–‡å†…å®¹ï¼‰"""
    issues = data.get('issues', [])
    extracted = []
    
    for issue in issues:
        extracted.append({
            'number': issue.get('number', 0),
            'title': issue.get('title', ''),
            'state': issue.get('state', ''),
            'comments': issue.get('comments', 0),
            'reactions': issue.get('reactions', {}).get('total_count', 0),
            'created_at': issue.get('created_at', ''),
            'updated_at': issue.get('updated_at', ''),
            'author': issue.get('user', {}).get('login', ''),
            'labels': [label.get('name', '') for label in issue.get('labels', [])],
            'body': issue.get('body', '') or '',  # å®Œæ•´ä¿ç•™æ­£æ–‡
        })
    
    return extracted


def extract_releases(data: Dict) -> List[Dict]:
    """æå– Releases ä¿¡æ¯ï¼ˆå®Œæ•´ä¿ç•™å‘å¸ƒè¯´æ˜ï¼‰"""
    releases = data.get('releases', [])
    extracted = []
    
    for release in releases:
        extracted.append({
            'tag_name': release.get('tag_name', ''),
            'name': release.get('name', ''),
            'published_at': release.get('published_at', ''),
            'author': release.get('author', {}).get('login', ''),
            'prerelease': release.get('prerelease', False),
            'body': release.get('body', '') or '',  # å®Œæ•´ä¿ç•™å‘å¸ƒè¯´æ˜
        })
    
    return extracted


def extract_docs(data: Dict) -> Dict[str, str]:
    """æå–æ–‡æ¡£å†…å®¹ï¼ˆå®Œæ•´ä¿ç•™æ‰€æœ‰å†…å®¹ï¼‰"""
    common_docs = data.get('common_docs', {})
    extracted = {}
    
    for doc_name, doc_data in common_docs.items():
        content = doc_data.get('decoded_content', '')
        if content:
            extracted[doc_name] = content  # å®Œæ•´ä¿ç•™å†…å®¹
    
    return extracted


def extract_workflows(data: Dict) -> List[str]:
    """æå–å·¥ä½œæµåç§°"""
    workflows = data.get('workflows', [])
    if not workflows:
        return []
    return [wf.get('name', 'unknown') for wf in workflows]


def extract_root_structure(data: Dict) -> List[Dict]:
    """æå–æ ¹ç›®å½•ç»“æ„"""
    root = data.get('root_contents', [])
    if not root:
        return []
    return [
        {'name': item.get('name', ''), 'type': item.get('type', '')}
        for item in root
    ]


def extract_docs_directory(data: Dict) -> List[str]:
    """æå– docs ç›®å½•ç»“æ„"""
    docs_dir = data.get('docs_directory', [])
    if not docs_dir:
        return []
    return [item.get('name', '') for item in docs_dir]


def extract_readme_links(data: Dict) -> List[Dict]:
    """æå– README ä¸­çš„é“¾æ¥"""
    return data.get('readme_links', [])


def extract_pr_template(data: Dict) -> str:
    """æå– PR æ¨¡æ¿å†…å®¹"""
    pr_template = data.get('pr_template', {})
    if pr_template:
        return pr_template.get('decoded_content', '')
    return ''


def extract_issue_templates(data: Dict) -> List[str]:
    """æå– Issue æ¨¡æ¿åç§°åˆ—è¡¨"""
    templates = data.get('issue_templates', [])
    if not templates:
        return []
    return [t.get('name', '') for t in templates]


def prepare_for_analysis(input_file: str, output_file: Optional[str] = None) -> Dict:
    """
    ä¸»å¤„ç†å‡½æ•°ï¼šä»å®Œæ•´ JSON æå–å…³é”®æ–‡æœ¬ä¿¡æ¯
    
    æ ¸å¿ƒåŸåˆ™ï¼š
    - å»é™¤æ— ç”¨å…ƒæ•°æ®ï¼ˆAPI URLsã€SHAã€node_id ç­‰ï¼‰
    - ä¿ç•™æ‰€æœ‰æœ‰ä»·å€¼çš„æ–‡æœ¬å†…å®¹ï¼ˆä¸æˆªæ–­ï¼‰
    - ç”Ÿæˆç»“æ„æ¸…æ™°çš„æ•°æ®
    
    Args:
        input_file: è¾“å…¥çš„å®Œæ•´ JSON æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºçš„ç²¾ç®€ JSON æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        ç²¾ç®€åçš„æ•°æ®å­—å…¸
    """
    # è¯»å–åŸå§‹æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–å…³é”®ä¿¡æ¯ï¼ˆå®Œæ•´ä¿ç•™æ–‡æœ¬å†…å®¹ï¼‰
    analysis_data = {
        'repository_url': data.get('repository_url', ''),
        'owner': data.get('owner', ''),
        'repo': data.get('repo', ''),
        'basic_info': extract_basic_info(data),
        'readme_content': extract_readme(data),
        'readme_links': extract_readme_links(data),
        'issues': extract_issues(data),
        'releases': extract_releases(data),
        'documents': extract_docs(data),
        'pr_template': extract_pr_template(data),
        'issue_templates': extract_issue_templates(data),
        'workflows': extract_workflows(data),
        'root_structure': extract_root_structure(data),
        'docs_directory': extract_docs_directory(data),
    }
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    analysis_data['stats'] = {
        'readme_length': len(analysis_data['readme_content']),
        'total_issues': len(analysis_data['issues']),
        'total_releases': len(analysis_data['releases']),
        'total_docs': len(analysis_data['documents']),
        'total_workflows': len(analysis_data['workflows']),
        'has_pr_template': bool(analysis_data['pr_template']),
        'has_issue_templates': len(analysis_data['issue_templates']) > 0,
        'has_docs_directory': len(analysis_data['docs_directory']) > 0,
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆ†æå°±ç»ªæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    return analysis_data


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä»å®Œæ•´ GitHub ä»“åº“ JSON æ•°æ®ä¸­æå–å…³é”®æ–‡æœ¬ä¿¡æ¯ï¼ˆå»é™¤æ— ç”¨å…ƒæ•°æ®ï¼Œä¿ç•™å®Œæ•´å†…å®¹ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  python prepare_for_analysis.py output/facebook_react/raw_data.json
  python prepare_for_analysis.py input.json -o output.json

æ³¨æ„ï¼šæ­¤è„šæœ¬åªå»é™¤æ— ç”¨çš„ JSON å…ƒæ•°æ®ï¼ˆå¦‚ API URLsã€SHA å€¼ç­‰ï¼‰ï¼Œ
æ‰€æœ‰æœ‰ä»·å€¼çš„æ–‡æœ¬å†…å®¹ï¼ˆREADMEã€Issue æ­£æ–‡ã€Release è¯´æ˜ç­‰ï¼‰éƒ½ä¼šå®Œæ•´ä¿ç•™ã€‚
        '''
    )
    
    parser.add_argument(
        'input_file',
        help='è¾“å…¥çš„å®Œæ•´ JSON æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '-o', '--output',
        help='è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º {input}_analysis_ready.jsonï¼‰'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    if args.output:
        output_file = args.output
    else:
        # ä»è¾“å…¥æ–‡ä»¶åæ¨æ–­
        input_path = Path(args.input_file)
        base_name = input_path.stem
        if base_name.endswith('_info'):
            base_name = base_name[:-5]
        elif base_name == 'raw_data':
            base_name = 'analysis_ready'
        else:
            base_name = base_name + '_analysis_ready'
        output_file = str(input_path.parent / f"{base_name}.json")
    
    # æ‰§è¡Œé¢„å¤„ç†
    result = prepare_for_analysis(args.input_file, output_file)
    
    # æ‰“å°ç»Ÿè®¡
    stats = result['stats']
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  â€¢ README: {stats['readme_length']:,} å­—ç¬¦")
    print(f"  â€¢ Issues: {stats['total_issues']} ä¸ªï¼ˆå®Œæ•´æ­£æ–‡ï¼‰")
    print(f"  â€¢ Releases: {stats['total_releases']} ä¸ªï¼ˆå®Œæ•´è¯´æ˜ï¼‰")
    print(f"  â€¢ æ–‡æ¡£: {stats['total_docs']} ä¸ªï¼ˆå®Œæ•´å†…å®¹ï¼‰")
    print(f"  â€¢ å·¥ä½œæµ: {stats['total_workflows']} ä¸ª")
    print(f"  â€¢ PR æ¨¡æ¿: {'æœ‰' if stats['has_pr_template'] else 'æ— '}")
    print(f"  â€¢ Issue æ¨¡æ¿: {'æœ‰' if stats['has_issue_templates'] else 'æ— '}")
    print(f"\nâœ¨ æ‰€æœ‰æ–‡æœ¬å†…å®¹å·²å®Œæ•´ä¿ç•™ï¼Œæ— æˆªæ–­å¤„ç†")


if __name__ == '__main__':
    main()
